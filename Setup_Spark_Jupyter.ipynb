{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spark cluster on EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note is on how to build a spark cluster on AWS ec2 from your jupyter notebook. Most of the tasks can be finished entirely within this notebook. However there are some dependencies and requirements that you will need to get ready:\n",
    "* Python 3.3+ on linux for this notebook to run\n",
    "* An AWS account with aws_access_key_id, and aws_secret_access_key\n",
    "* Installed aws_cli\n",
    "* boto3\n",
    "\n",
    "ref: http://blog.insightdatalabs.com/spark-cluster-step-by-step/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boto3 is a handy aws resource management tool. We use it to build a simple cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install aws\n",
    "# run 'aws configure' in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check your aws account credentials **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define two handy functions that we use often: check_status, and terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_status(ec2, instances):\n",
    "    \"\"\"check statuses of instances \n",
    "    \"\"\"\n",
    "    for instance in instances:\n",
    "        print(\"{} is {}\".format(instance.id, \n",
    "                                ec2.Instance(id=instance.id).state['Name']))\n",
    "        \n",
    "def terminate(instances):\n",
    "    \"\"\"terminate instances \n",
    "    \"\"\"    \n",
    "    for instance in instances:\n",
    "        instance.terminate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configurations\n",
    "ubuntu14lts = 'ami-5ac2cd4d'  # image Ubuntu Server 14.04 LTS (HVM), SSD Volume Type\n",
    "num_nodes = 4\n",
    "instance_type = 'm4.large'\n",
    "security_group_name = 'jupyter-spark' # security group name\n",
    "key_pair_name = 'jupyter' # name for your pem file, will store in ~/.aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2')\n",
    "client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Security group: open to all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For similicity, we allow all inbound traffic from anywhere. To be safe, worker nodes need to be carefully modified to only accept internal traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'date': 'Wed, 21 Dec 2016 20:47:39 GMT',\n",
       "   'server': 'AmazonEC2',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'vary': 'Accept-Encoding'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'a48420c0-0c77-4507-b82a-3214bf7429c7',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a special security group \n",
    "response = ec2.create_security_group(GroupName=security_group_name,\n",
    "                                     Description='security group for jupyter notebooks traffic')\n",
    "# allow port 22 for ssh\n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=22,\n",
    "                                     ToPort=22) \n",
    "# allow port 8080  \n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=8080,\n",
    "                                     ToPort=8080) \n",
    "\n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=0,\n",
    "                                     ToPort=65565) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new pem file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ddu/.aws/jupyter.pem already exists.\n"
     ]
    }
   ],
   "source": [
    "# create a new key pair (.pem)\n",
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "kp_path = expanduser(\"~\") + '/.aws/'+key_pair_name + '.pem'\n",
    "if not Path(kp_path).is_file():\n",
    "    key_pair = client.create_key_pair(KeyName=key_pair_name)\n",
    "    with open(kp_path,'w') as wt:\n",
    "        wt.write(key_pair['KeyMaterial'])\n",
    "else:\n",
    "    print(\"{} already exists.\".format(kp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod 600 {kp_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands to run at launch for all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let all the nodes run the following commands upon spinning up. <br>\n",
    "It will perform the following tasks:\n",
    "* Install java-8\n",
    "* Download spark-1.6.3 and install\n",
    "* Create ~/.profile and set PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/bin/bash\\nsudo add-apt-repository ppa:openjdk-r/ppa \\nsudo apt-get update \\nsudo apt-get install -y openjdk-8-jdk\\nsudo apt-get install scala\\nwget http://apache.mirrors.tds.net/spark/spark-1.6.3/spark-1.6.3-bin-hadoop2.4.tgz -P ~/Downloads\\nsudo tar zxvf ~/Downloads/spark-* -C /usr/local\\nsudo mv /usr/local/spark-* /usr/local/spark\\n\\nsudo cat << EOF >> /home/ubuntu/.profile\\nexport SPARK_HOME=/usr/local/spark  \\nexport PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNSPARK_HOME/bin  \\nEOF\\n\\nsed -i -e \\'s/DOLLAR_SIGN/$/g\\' /home/ubuntu/.profile\\nsource /home/ubuntu/.profile\\nsudo chown -R ubuntu $SPARK_HOME  \\n\\necho \"#!/usr/bin/env bash\" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export JAVA_HOME=/usr\" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export SPARK_PUBLIC_DNS=\\'current_node_public_dns\\' \" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export SPARK_WORKER_CORES=6\" >> $SPARK_HOME/conf/spark-env.sh\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data = '''#!/bin/bash\n",
    "sudo add-apt-repository ppa:openjdk-r/ppa \n",
    "sudo apt-get update \n",
    "sudo apt-get install -y openjdk-8-jdk\n",
    "sudo apt-get install scala\n",
    "wget http://apache.mirrors.tds.net/spark/spark-1.6.3/spark-1.6.3-bin-hadoop2.4.tgz -P ~/Downloads\n",
    "sudo tar zxvf ~/Downloads/spark-* -C /usr/local\n",
    "sudo mv /usr/local/spark-* /usr/local/spark\n",
    "\n",
    "sudo cat << EOF >> /home/ubuntu/.profile\n",
    "export SPARK_HOME=/usr/local/spark  \n",
    "export PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNSPARK_HOME/bin  \n",
    "EOF\n",
    "\n",
    "sed -i -e 's/DOLLAR_SIGN/$/g' /home/ubuntu/.profile\n",
    "source /home/ubuntu/.profile\n",
    "sudo chown -R ubuntu $SPARK_HOME  \n",
    "\n",
    "echo \"#!/usr/bin/env bash\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export JAVA_HOME=/usr\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export SPARK_PUBLIC_DNS='current_node_public_dns' \" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export SPARK_WORKER_CORES={}\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "'''.format( (num_nodes - 1)*2 )\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Send out request to create instances, finally!!\n",
    "\n",
    "For more details on other options (for example spot instances), pls refer the boto3 documentation linked below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DeviceName': '/dev/sda1',\n",
       " 'Ebs': {'DeleteOnTermination': True,\n",
       "  'VolumeSize': 200,\n",
       "  'VolumeType': 'standard'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volumns = { 'DeviceName': '/dev/sda1',\n",
    "            'Ebs': { 'VolumeSize': 200,\n",
    "                     'DeleteOnTermination': True,\n",
    "                     'VolumeType': 'standard', # Magnetic\n",
    "                    },\n",
    "          }\n",
    "volumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create instances\n",
    "instances = ec2.create_instances(ImageId = ubuntu14lts,\n",
    "                                 SecurityGroups = [ security_group_name ],\n",
    "                                 InstanceType = instance_type, \n",
    "                                 KeyName = key_pair_name,\n",
    "                                 MinCount = num_nodes,\n",
    "                                 MaxCount = num_nodes,\n",
    "                                 UserData = user_data,    \n",
    "                                 BlockDeviceMappings=[volumns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check this a couple of times until all are running/or terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-0de4771761532efa7 is terminated\n",
      "i-033e623d21d818abb is terminated\n",
      "i-0ae5a7a81aad61775 is terminated\n",
      "i-0523d0de5abaf9c2b is terminated\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BE CAUTIOUS!! This will kill the whole cluster\n",
    "terminate(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-88-158-48.compute-1.amazonaws.com\n",
      "ec2-54-196-73-124.compute-1.amazonaws.com\n",
      "ec2-54-152-115-105.compute-1.amazonaws.com\n",
      "ec2-54-145-234-70.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# we need a bunch of parameters that will be needed below\n",
    "# We will set the first node to be the master, and others are slaves\n",
    "pub_dns = []\n",
    "ins_ids = []\n",
    "pri_dns = []\n",
    "for instance in instances:\n",
    "    #print(\"{} is {}\".format(instance.id, ec2.Instance(id=instance.id).state['Name']))\n",
    "    pub_dns.append(ec2.Instance(id=instance.id).public_dns_name)\n",
    "    ins_ids.append(instance.id)\n",
    "    pri_dns.append(ec2.Instance(id=instance.id).private_dns_name.split('.')[0])\n",
    "    print(ec2.Instance(id=instance.id).public_dns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Now, we need to do a bunch of modifications to the spark config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "jupyter.pem                                     0%    0     0.0KB/s   --:-- ETA\r",
      "jupyter.pem                                   100% 1670     1.6KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "# first, upload a copy of pem file to master\n",
    "!scp -i {kp_path} -o 'StrictHostKeyChecking no' {kp_path} ubuntu@{pub_dns[0]}:/home/ubuntu/.ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ssh_cmd: cmd to update ssh\n",
    "\n",
    "pem_key_file = '/home/ubuntu/.ssh/'+key_pair_name+'.pem'\n",
    "ssh_cmd = '''\n",
    "cat << EOF | tee /home/ubuntu/.ssh/config\n",
    "Host namenode\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "'''.format( pub_dns[0], pem_key_file)\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "Host datanode{}\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "    '''.format(i+1, dns, pem_key_file)\n",
    "ssh_cmd += '''\n",
    "EOF\n",
    "\n",
    "ssh-keygen -f /home/ubuntu/.ssh/id_rsa -t rsa -P ''\n",
    "cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys\n",
    "'''\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "    cat /home/ubuntu/.ssh/id_rsa.pub | ssh -o 'StrictHostKeyChecking no' datanode{} 'cat >> /home/ubuntu/.ssh/authorized_keys'\n",
    "    '''.format(i+1)\n",
    "#ssh_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host namenode\n",
      "  HostName ec2-54-88-158-48.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "\n",
      "Host datanode1\n",
      "  HostName ec2-54-196-73-124.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode2\n",
      "  HostName ec2-54-152-115-105.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode3\n",
      "  HostName ec2-54-145-234-70.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\n",
      "Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\n",
      "The key fingerprint is:\n",
      "96:d4:68:c7:ed:aa:45:36:78:9b:a2:1c:a4:0f:0e:68 ubuntu@ip-172-31-28-152\n",
      "The key's randomart image is:\n",
      "+--[ RSA 2048]----+\n",
      "|                 |\n",
      "|         + .     |\n",
      "|        + + .    |\n",
      "|       o + .     |\n",
      "|      . S = .    |\n",
      "| .   o . + =     |\n",
      "|.E. o . . =      |\n",
      "|.  o + o +       |\n",
      "|    . + .        |\n",
      "+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# update the ssh config at master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{ssh_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now update the master node config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sudo rm -rf \\\\$SPARK_HOME/conf/slaves;  source /home/ubuntu/.profile; sudo echo \"ec2-54-196-73-124.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; sudo echo \"ec2-54-152-115-105.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; sudo echo \"ec2-54-145-234-70.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master_cmd: modify master \n",
    "\n",
    "master_cmd = ' sudo rm -rf \\$SPARK_HOME/conf/slaves; '\n",
    "master_cmd += ' source /home/ubuntu/.profile; '\n",
    "for slave in pub_dns[1:]:\n",
    "    master_cmd += 'sudo echo \"{}\" | sudo tee --append \\$SPARK_HOME/conf/slaves; '.format(slave)\n",
    "master_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-196-73-124.compute-1.amazonaws.com\r\n",
      "ec2-54-152-115-105.compute-1.amazonaws.com\r\n",
      "ec2-54-145-234-70.compute-1.amazonaws.com\r\n"
     ]
    }
   ],
   "source": [
    "# modify master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{master_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurrah! You are ready to run your spark cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use your termininal to start the cluster <br>\n",
    "Open a terminal, ssh to your master node <br>\n",
    "And run the commands:\n",
    "\n",
    "```bash\n",
    "master$ $SPARK_HOME/sbin/start-all.sh\n",
    "```\n",
    "\n",
    "In case to stop,\n",
    "```bash\n",
    "master$ $SPARK_HOME/sbin/stop-all.sh\n",
    "```\n",
    "\n",
    "For more details, check out Ouyang's post: <br>\n",
    "http://blog.insightdatalabs.com/spark-cluster-step-by-step/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to this place to find the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ec2-54-88-158-48.compute-1.amazonaws.com:8080'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://{}:8080'.format(pub_dns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terminate(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-0de4771761532efa7 is shutting-down\n",
      "i-033e623d21d818abb is shutting-down\n",
      "i-0ae5a7a81aad61775 is shutting-down\n",
      "i-0523d0de5abaf9c2b is shutting-down\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsny]",
   "language": "python",
   "name": "conda-env-dsny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
