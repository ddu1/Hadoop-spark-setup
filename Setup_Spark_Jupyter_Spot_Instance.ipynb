{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spark cluster on EC2 (Spot instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note is on how to build a spark cluster on AWS ec2 from your jupyter notebook. Most of the tasks can be finished entirely within this notebook. However there are some dependencies and requirements that you will need to get ready:\n",
    "* Python 3.3+ on linux for this notebook to run\n",
    "* An AWS account with aws_access_key_id, and aws_secret_access_key\n",
    "* Installed aws_cli\n",
    "* boto3\n",
    "\n",
    "ref: http://blog.insightdatalabs.com/spark-cluster-step-by-step/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boto3 is a handy aws resource management tool. We use it to build a simple cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install aws\n",
    "# run 'aws configure' in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check your aws account credentials **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define two handy functions that we use often: check_status, and terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_status(ec2, ins_ids):\n",
    "    \"\"\"check statuses of instances \n",
    "    \"\"\"\n",
    "    for instance_id in ins_ids:\n",
    "        print(\"{} is {}\".format(instance_id, \n",
    "                                ec2.Instance(id=instance_id).state['Name']))\n",
    "        \n",
    "def terminate(ec2, ins_ids):\n",
    "    \"\"\"terminate instances \n",
    "    \"\"\"    \n",
    "    for instance_id in ins_ids:\n",
    "        ec2.Instance(id=instance_id).terminate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configurations\n",
    "ubuntu14lts = 'ami-5ac2cd4d'  # image Ubuntu Server 14.04 LTS (HVM), SSD Volume Type\n",
    "num_nodes = 4\n",
    "instance_type = 'm4.large'\n",
    "security_group_name = 'jupyter-spark-all' # security group name\n",
    "key_pair_name = 'jupyter' # name for your pem file, will store in ~/.aws/\n",
    "spot_price = '0.02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2')\n",
    "client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Security group: open to all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For similicity, we allow all inbound traffic from anywhere. To be safe, worker nodes need to be carefully modified to only accept internal traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'date': 'Wed, 21 Dec 2016 23:46:40 GMT',\n",
       "   'server': 'AmazonEC2',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'vary': 'Accept-Encoding'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'e8cf1229-4e98-4414-9393-00bfb269b758',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a special security group \n",
    "response = ec2.create_security_group(GroupName=security_group_name,\n",
    "                                     Description='security group for jupyter notebooks traffic')\n",
    "# allow port 22 for ssh\n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=22,\n",
    "                                     ToPort=22) \n",
    "# allow port 8080  \n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=8080,\n",
    "                                     ToPort=8080) \n",
    "\n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol='-1',\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=1,\n",
    "                                     ToPort=65535) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new pem file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ddu/.aws/jupyter.pem already exists.\n"
     ]
    }
   ],
   "source": [
    "# create a new key pair (.pem)\n",
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "kp_path = expanduser(\"~\") + '/.aws/'+key_pair_name + '.pem'\n",
    "if not Path(kp_path).is_file():\n",
    "    key_pair = client.create_key_pair(KeyName=key_pair_name)\n",
    "    with open(kp_path,'w') as wt:\n",
    "        wt.write(key_pair['KeyMaterial'])\n",
    "else:\n",
    "    print(\"{} already exists.\".format(kp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod 600 {kp_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands to run at launch for all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let all the nodes run the following commands upon spinning up. <br>\n",
    "It will perform the following tasks:\n",
    "* Install java-8\n",
    "* Download spark-1.6.3 and install\n",
    "* Create ~/.profile and set PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/bin/bash\\nsudo add-apt-repository ppa:openjdk-r/ppa \\nsudo apt-get update \\nsudo apt-get install -y openjdk-8-jdk\\nsudo apt-get install scala\\nwget http://apache.mirrors.tds.net/spark/spark-1.6.3/spark-1.6.3-bin-hadoop2.4.tgz -P ~/Downloads\\nsudo tar zxvf ~/Downloads/spark-* -C /usr/local\\nsudo mv /usr/local/spark-* /usr/local/spark\\n\\nsudo cat << EOF >> /home/ubuntu/.profile\\nexport SPARK_HOME=/usr/local/spark  \\nexport PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNSPARK_HOME/bin  \\nEOF\\n\\nsed -i -e \\'s/DOLLAR_SIGN/$/g\\' /home/ubuntu/.profile\\nsource /home/ubuntu/.profile\\nsudo chown -R ubuntu $SPARK_HOME  \\n\\necho \"#!/usr/bin/env bash\" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export JAVA_HOME=/usr\" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export SPARK_PUBLIC_DNS=\\'current_node_public_dns\\' \" >> $SPARK_HOME/conf/spark-env.sh\\necho \"export SPARK_WORKER_CORES=6\" >> $SPARK_HOME/conf/spark-env.sh\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data = '''#!/bin/bash\n",
    "sudo add-apt-repository ppa:openjdk-r/ppa \n",
    "sudo apt-get update \n",
    "sudo apt-get install -y openjdk-8-jdk\n",
    "sudo apt-get install scala\n",
    "wget http://apache.mirrors.tds.net/spark/spark-1.6.3/spark-1.6.3-bin-hadoop2.4.tgz -P ~/Downloads\n",
    "sudo tar zxvf ~/Downloads/spark-* -C /usr/local\n",
    "sudo mv /usr/local/spark-* /usr/local/spark\n",
    "\n",
    "sudo cat << EOF >> /home/ubuntu/.profile\n",
    "export SPARK_HOME=/usr/local/spark  \n",
    "export PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNSPARK_HOME/bin  \n",
    "EOF\n",
    "\n",
    "sed -i -e 's/DOLLAR_SIGN/$/g' /home/ubuntu/.profile\n",
    "source /home/ubuntu/.profile\n",
    "sudo chown -R ubuntu $SPARK_HOME  \n",
    "\n",
    "echo \"#!/usr/bin/env bash\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export JAVA_HOME=/usr\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export SPARK_PUBLIC_DNS='current_node_public_dns' \" >> $SPARK_HOME/conf/spark-env.sh\n",
    "echo \"export SPARK_WORKER_CORES={}\" >> $SPARK_HOME/conf/spark-env.sh\n",
    "'''.format( (num_nodes - 1)*2 )\n",
    "user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IyEvYmluL2Jhc2gKc3VkbyBhZGQtYXB0LXJlcG9zaXRvcnkgcHBhOm9wZW5qZGstci9wcGEgCnN1ZG8gYXB0LWdldCB1cGRhdGUgCnN1ZG8gYXB0LWdldCBpbnN0YWxsIC15IG9wZW5qZGstOC1qZGsKc3VkbyBhcHQtZ2V0IGluc3RhbGwgc2NhbGEKd2dldCBodHRwOi8vYXBhY2hlLm1pcnJvcnMudGRzLm5ldC9zcGFyay9zcGFyay0xLjYuMy9zcGFyay0xLjYuMy1iaW4taGFkb29wMi40LnRneiAtUCB+L0Rvd25sb2FkcwpzdWRvIHRhciB6eHZmIH4vRG93bmxvYWRzL3NwYXJrLSogLUMgL3Vzci9sb2NhbApzdWRvIG12IC91c3IvbG9jYWwvc3BhcmstKiAvdXNyL2xvY2FsL3NwYXJrCgpzdWRvIGNhdCA8PCBFT0YgPj4gL2hvbWUvdWJ1bnR1Ly5wcm9maWxlCmV4cG9ydCBTUEFSS19IT01FPS91c3IvbG9jYWwvc3BhcmsgIApleHBvcnQgUEFUSD1ET0xMQVJfU0lHTlBBVEg6RE9MTEFSX1NJR05TUEFSS19IT01FL2JpbiAgCkVPRgoKc2VkIC1pIC1lICdzL0RPTExBUl9TSUdOLyQvZycgL2hvbWUvdWJ1bnR1Ly5wcm9maWxlCnNvdXJjZSAvaG9tZS91YnVudHUvLnByb2ZpbGUKc3VkbyBjaG93biAtUiB1YnVudHUgJFNQQVJLX0hPTUUgIAoKZWNobyAiIyEvdXNyL2Jpbi9lbnYgYmFzaCIgPj4gJFNQQVJLX0hPTUUvY29uZi9zcGFyay1lbnYuc2gKZWNobyAiZXhwb3J0IEpBVkFfSE9NRT0vdXNyIiA+PiAkU1BBUktfSE9NRS9jb25mL3NwYXJrLWVudi5zaAplY2hvICJleHBvcnQgU1BBUktfUFVCTElDX0ROUz0nY3VycmVudF9ub2RlX3B1YmxpY19kbnMnICIgPj4gJFNQQVJLX0hPTUUvY29uZi9zcGFyay1lbnYuc2gKZWNobyAiZXhwb3J0IFNQQVJLX1dPUktFUl9DT1JFUz02IiA+PiAkU1BBUktfSE9NRS9jb25mL3NwYXJrLWVudi5zaAo='"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "user_data_encoded = base64.b64encode(user_data.encode('utf-8'))\n",
    "user_data_encoded.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Send out request to create instances, finally!!\n",
    "\n",
    "For more details on other options (for example spot instances), pls refer the boto3 documentation linked below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DeviceName': '/dev/sda1',\n",
       " 'Ebs': {'DeleteOnTermination': True,\n",
       "  'VolumeSize': 200,\n",
       "  'VolumeType': 'standard'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volumns = { 'DeviceName': '/dev/sda1',\n",
    "            'Ebs': { 'VolumeSize': 200,\n",
    "                     'DeleteOnTermination': True,\n",
    "                     'VolumeType': 'standard', # Magnetic\n",
    "                    },\n",
    "          }\n",
    "volumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create instances\n",
    "#instances = ec2.create_instances(ImageId = ubuntu14lts,\n",
    "#                                 SecurityGroups = [ security_group_name ],\n",
    "#                                 InstanceType = instance_type, \n",
    "#                                 KeyName = key_pair_name,\n",
    "#                                 MinCount = num_nodes,\n",
    "#                                 MaxCount = num_nodes,\n",
    "#                                 UserData = user_data,    \n",
    "#                                 BlockDeviceMappings=[volumns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spot instance\n",
    "response = client.request_spot_instances(\n",
    "    SpotPrice=spot_price,\n",
    "    Type='one-time', #|'persistent',\n",
    "    InstanceCount=num_nodes,\n",
    "    LaunchSpecification={\n",
    "        'ImageId': ubuntu14lts,\n",
    "        'KeyName': key_pair_name,\n",
    "        'SecurityGroups': [\n",
    "            security_group_name,\n",
    "        ],\n",
    "        'UserData': user_data_encoded.decode('utf-8'),\n",
    "        'InstanceType': instance_type,\n",
    "\n",
    "        'BlockDeviceMappings': [ volumns],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_spot_instances(response):\n",
    "    req_ids = []\n",
    "    ins_ids = []\n",
    "    for ins in response['SpotInstanceRequests']:\n",
    "        req_ids.append(ins['SpotInstanceRequestId'])  \n",
    "    res = client.describe_spot_instance_requests( SpotInstanceRequestIds=req_ids )\n",
    "    for i,ins in enumerate(res['SpotInstanceRequests']):\n",
    "        print('{} {}'.format(req_ids[i],ins['Status']['Code']))\n",
    "        #print(ins)\n",
    "        if 'InstanceId' in ins:\n",
    "            ins_ids.append(ins['InstanceId'])\n",
    "    return ins_ids\n",
    "    #print(res['SpotInstanceRequests'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sir-3vbg78kk fulfilled\n",
      "sir-cq184ymk fulfilled\n",
      "sir-hvrg7zej fulfilled\n",
      "sir-ww4g7c7j fulfilled\n"
     ]
    }
   ],
   "source": [
    "ins_ids = check_spot_instances(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check this a couple of times until all are running/or terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-09dbbb009395ededa is running\n",
      "i-06e44cd235931c050 is running\n",
      "i-0e5f40df170d9cdc8 is running\n",
      "i-0468a79307e26c9a3 is running\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, ins_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BE CAUTIOUS!! This will kill the whole cluster\n",
    "terminate_spot_instances(ec2, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-227-16-198.compute-1.amazonaws.com\n",
      "ec2-54-161-48-132.compute-1.amazonaws.com\n",
      "ec2-54-144-23-151.compute-1.amazonaws.com\n",
      "ec2-54-227-162-108.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# we need a bunch of parameters that will be needed below\n",
    "# We will set the first node to be the master, and others are slaves\n",
    "pub_dns = []\n",
    "pri_dns = []\n",
    "for idx in ins_ids:\n",
    "    #print(\"{} is {}\".format(instance.id, ec2.Instance(id=instance.id).state['Name']))\n",
    "    pub_dns.append(ec2.Instance(id=idx).public_dns_name)\n",
    "    pri_dns.append(ec2.Instance(id=idx).private_dns_name.split('.')[0])\n",
    "    print(ec2.Instance(id=idx).public_dns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Now, we need to do a bunch of modifications to the spark config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'ec2-54-227-16-198.compute-1.amazonaws.com,54.227.16.198' (ECDSA) to the list of known hosts.\n",
      "jupyter.pem                                   100% 1670     1.6KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "# first, upload a copy of pem file to master\n",
    "!scp -i {kp_path} -o 'StrictHostKeyChecking no' {kp_path} ubuntu@{pub_dns[0]}:/home/ubuntu/.ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ssh_cmd: cmd to update ssh\n",
    "\n",
    "pem_key_file = '/home/ubuntu/.ssh/'+key_pair_name+'.pem'\n",
    "ssh_cmd = '''\n",
    "cat << EOF | tee /home/ubuntu/.ssh/config\n",
    "Host namenode\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "'''.format( pub_dns[0], pem_key_file)\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "Host datanode{}\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "    '''.format(i+1, dns, pem_key_file)\n",
    "ssh_cmd += '''\n",
    "EOF\n",
    "\n",
    "ssh-keygen -f /home/ubuntu/.ssh/id_rsa -t rsa -P ''\n",
    "cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys\n",
    "'''\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "    cat /home/ubuntu/.ssh/id_rsa.pub | ssh -o 'StrictHostKeyChecking no' datanode{} 'cat >> /home/ubuntu/.ssh/authorized_keys'\n",
    "    '''.format(i+1)\n",
    "#ssh_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host namenode\n",
      "  HostName ec2-54-227-16-198.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "\n",
      "Host datanode1\n",
      "  HostName ec2-54-161-48-132.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode2\n",
      "  HostName ec2-54-144-23-151.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode3\n",
      "  HostName ec2-54-227-162-108.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\n",
      "Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\n",
      "The key fingerprint is:\n",
      "82:be:37:78:77:f4:bf:20:6f:0a:2f:35:94:81:76:9b ubuntu@ip-172-31-8-85\n",
      "The key's randomart image is:\n",
      "+--[ RSA 2048]----+\n",
      "|         .       |\n",
      "|        o o      |\n",
      "|       . . =     |\n",
      "|     .    E      |\n",
      "|    . . S.       |\n",
      "|   .   .  +      |\n",
      "|    ..  .o.o.    |\n",
      "|    ..+ oo.oo.   |\n",
      "|    .o o ooo..o. |\n",
      "+-----------------+\n",
      "Warning: Permanently added 'ec2-54-161-48-132.compute-1.amazonaws.com,172.31.14.86' (ECDSA) to the list of known hosts.\n",
      "Warning: Permanently added 'ec2-54-144-23-151.compute-1.amazonaws.com,172.31.0.127' (ECDSA) to the list of known hosts.\n",
      "Warning: Permanently added 'ec2-54-227-162-108.compute-1.amazonaws.com,172.31.1.22' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "# update the ssh config at master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{ssh_cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-227-16-198.compute-1.amazonaws.com\n",
      "ec2-54-161-48-132.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-54-161-48-132.compute-1.amazonaws.com,54.161.48.132' (ECDSA) to the list of known hosts.\n",
      "ec2-54-144-23-151.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-54-144-23-151.compute-1.amazonaws.com,54.144.23.151' (ECDSA) to the list of known hosts.\n",
      "ec2-54-227-162-108.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-54-227-162-108.compute-1.amazonaws.com,54.227.162.108' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "# update each node (include master)\n",
    "for everybody in pub_dns: # \n",
    "    update_cmd = 'source /home/ubuntu/.profile; '\n",
    "    update_cmd += 'sudo sed -i \"s/current_node_public_dns/{}/g\" \\$SPARK_HOME/conf/spark-env.sh;'.format(everybody)\n",
    "    print(everybody)\n",
    "    !ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{everybody} \"{update_cmd}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now update the master node config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sudo rm -rf \\\\$SPARK_HOME/conf/slaves;  source /home/ubuntu/.profile; sudo echo \"ec2-54-161-48-132.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; sudo echo \"ec2-54-144-23-151.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; sudo echo \"ec2-54-227-162-108.compute-1.amazonaws.com\" | sudo tee --append \\\\$SPARK_HOME/conf/slaves; '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master_cmd: modify master \n",
    "\n",
    "master_cmd = ' sudo rm -rf \\$SPARK_HOME/conf/slaves; '\n",
    "master_cmd += ' source /home/ubuntu/.profile; '\n",
    "for slave in pub_dns[1:]:\n",
    "    master_cmd += 'sudo echo \"{}\" | sudo tee --append \\$SPARK_HOME/conf/slaves; '.format(slave)\n",
    "master_cmd += ' sudo sed -i -e \"s/current_node_public_dns/{}/g\" \\$SPARK_HOME/conf/spark-env.sh; '\n",
    "master_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-161-48-132.compute-1.amazonaws.com\r\n",
      "ec2-54-144-23-151.compute-1.amazonaws.com\r\n",
      "ec2-54-227-162-108.compute-1.amazonaws.com\r\n"
     ]
    }
   ],
   "source": [
    "# modify master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{master_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurrah! You are ready to run your spark cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use your termininal to start the cluster <br>\n",
    "Open a terminal, ssh to your master node <br>\n",
    "And run the commands:\n",
    "\n",
    "```bash\n",
    "master$ $SPARK_HOME/sbin/start-all.sh\n",
    "```\n",
    "\n",
    "In case to stop,\n",
    "```bash\n",
    "master$ $SPARK_HOME/sbin/stop-all.sh\n",
    "```\n",
    "\n",
    "For more details, check out Ouyang's post: <br>\n",
    "http://blog.insightdatalabs.com/spark-cluster-step-by-step/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to this place to find the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-ip-172-31-8-85.out\n",
      "ec2-54-161-48-132.compute-1.amazonaws.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-ip-172-31-14-86.out\n",
      "ec2-54-227-162-108.compute-1.amazonaws.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-ip-172-31-1-22.out\n",
      "ec2-54-144-23-151.compute-1.amazonaws.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-ip-172-31-0-127.out\n"
     ]
    }
   ],
   "source": [
    "start_cmd = ' source /home/ubuntu/.profile; \\$SPARK_HOME/sbin/start-all.sh; '\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{start_cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-227-162-108.compute-1.amazonaws.com: stopping org.apache.spark.deploy.worker.Worker\n",
      "ec2-54-161-48-132.compute-1.amazonaws.com: stopping org.apache.spark.deploy.worker.Worker\n",
      "ec2-54-144-23-151.compute-1.amazonaws.com: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "stop_cmd = ' source /home/ubuntu/.profile; \\$SPARK_HOME/sbin/stop-all.sh; '\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{stop_cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ec2-54-227-16-198.compute-1.amazonaws.com:8080'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://{}:8080'.format(pub_dns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup to tunnel to local jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get aws_access_key_id and aws_secret_access_key from your aws credentials\n",
    "%load /home/ddu/.aws/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook.auth import passwd\n",
    "notebook_pass = '123456'\n",
    "encrypted = passwd(notebook_pass, algorithm='sha1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sudo apt-get install -y python-dev python-pip python-numpy python-scipy python-pandas gfortran; sudo pip install nose \"ipython[notebook]\" ; cat << EOF | tee --append /home/ubuntu/.profile\\nexport AWS_ACCESS_KEY_ID=AKIAI374YOEQVFDUFCEA\\nexport AWS_SECRET_ACCESS_KEY=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\\nEOF\\n\\nmkdir -p /home/ubuntu/.jupyter\\nif [ ! -f \"/home/ubuntu/.jupyter/jupyter_notebook_config.py\" ]; then\\ncat << EOF | tee /home/ubuntu/.jupyter/jupyter_notebook_config.py\\nc.NotebookApp.ip = \\'*\\'\\nc.NotebookApp.password = sha1:9f3a012efb18:23c9567caba001c35910efb7d1cea0e7a9f093bb\\nc.NotebookApp.open_browser = False\\n# It is a good idea to set a known, fixed port for server access\\nc.NotebookApp.port = 8888\\nEOF\\nfi\\n\\nPYSPARK_DRIVER_PYTHON=ipython\\ntmux\\njupyter notebook\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install ipython\n",
    "master_cmd = ''\n",
    "master_cmd += 'sudo apt-get install -y python-dev python-pip python-numpy python-scipy python-pandas gfortran; '\n",
    "master_cmd += 'sudo pip install nose \"ipython[notebook]\" ; '\n",
    "master_cmd += '''cat << EOF | tee --append /home/ubuntu/.profile\n",
    "export AWS_ACCESS_KEY_ID={}\n",
    "export AWS_SECRET_ACCESS_KEY={}\n",
    "EOF\n",
    "\n",
    "mkdir -p /home/ubuntu/.jupyter\n",
    "if [ ! -f \"/home/ubuntu/.jupyter/jupyter_notebook_config.py\" ]; then\n",
    "cat << EOF | tee /home/ubuntu/.jupyter/jupyter_notebook_config.py\n",
    "c.NotebookApp.ip = '*'\n",
    "c.NotebookApp.password = {}\n",
    "c.NotebookApp.open_browser = False\n",
    "# It is a good idea to set a known, fixed port for server access\n",
    "c.NotebookApp.port = 8888\n",
    "EOF\n",
    "fi\n",
    "\n",
    "PYSPARK_DRIVER_PYTHON=ipython\n",
    "tmux\n",
    "jupyter notebook\n",
    "\n",
    "'''.format( aws_access_key_id,  aws_secret_access_key, encrypted)\n",
    "master_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{master_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terminate(ec2, ins_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-012981a062f8abaa3 is shutting-down\n",
      "i-090d3d3075ae24f14 is shutting-down\n",
      "i-095864467acef2fe3 is shutting-down\n",
      "i-038da90f77029607a is shutting-down\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, ins_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsny]",
   "language": "python",
   "name": "conda-env-dsny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
