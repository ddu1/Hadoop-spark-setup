{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Hadoop cluster on EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note is on how to build a hadoop cluster on AWS ec2 from your jupyter notebook. Most of the tasks can be finished entirely within this notebook. However there are some dependencies and requirements that you will need to get ready:\n",
    "* Python 3.3+ on linux for this notebook to run\n",
    "* An AWS account with aws_access_key_id, and aws_secret_access_key\n",
    "* Installed aws_cli\n",
    "* boto3\n",
    "\n",
    "ref: https://blog.insightdatascience.com/spinning-up-a-free-hadoop-cluster-step-by-step-c406d56bae42#.j3zrw1yua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boto3 is a handy aws resource management tool. We use it to build a simple cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install aws\n",
    "# run 'aws configure' in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check your aws account credentials **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define two handy functions that we use often: check_status, and terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_status(ec2, instances):\n",
    "    \"\"\"check statuses of instances \n",
    "    \"\"\"\n",
    "    for instance in instances:\n",
    "        print(\"{} is {}\".format(instance.id, \n",
    "                                ec2.Instance(id=instance.id).state['Name']))\n",
    "        \n",
    "def terminate(instances):\n",
    "    \"\"\"terminate instances \n",
    "    \"\"\"    \n",
    "    for instance in instances:\n",
    "        instance.terminate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configurations\n",
    "ubuntu14lts = 'ami-5ac2cd4d'  # image Ubuntu Server 14.04 LTS (HVM), SSD Volume Type\n",
    "num_nodes = 4\n",
    "instance_type = 't2.micro'\n",
    "security_group_name = 'jupyter-users-open' # security group name\n",
    "key_pair_name = 'jupyter' # name for your pem file, will store in ~/.aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2')\n",
    "client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Security group: open to all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'date': 'Wed, 21 Dec 2016 18:22:48 GMT',\n",
       "   'server': 'AmazonEC2',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'vary': 'Accept-Encoding'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': '70423049-1824-4c57-a764-1720571de12f',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a special security group \n",
    "response = ec2.create_security_group(GroupName=security_group_name,\n",
    "                                     Description='security group for jupyter notebooks traffic')\n",
    "# allow port 22 for ssh\n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=22,\n",
    "                                     ToPort=22) \n",
    "# allow port 50070  \n",
    "client.authorize_security_group_ingress(GroupId=response.group_id,\n",
    "                                     IpProtocol=\"tcp\",\n",
    "                                     CidrIp=\"0.0.0.0/0\",\n",
    "                                     FromPort=50070,\n",
    "                                     ToPort=50070) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new pem file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new key pair (.pem)\n",
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "kp_path = expanduser(\"~\") + '/.aws/'+key_pair_name + '.pem'\n",
    "if not Path(kp_path).is_file():\n",
    "    key_pair = client.create_key_pair(KeyName=key_pair_name)\n",
    "    with open(kp_path,'w') as wt:\n",
    "        wt.write(key_pair['KeyMaterial'])\n",
    "else:\n",
    "    print(\"{} already exists.\".format(kp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod 600 {kp_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands to run at launch for all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let all the nodes run the following commands upon spinning up. <br>\n",
    "It will perform the following tasks:\n",
    "* Install java-8\n",
    "* Download hadoop-2.7 and install\n",
    "* Create ~/.profile and set PATH\n",
    "* Setup a bunch of Hadoop config files that we later will update once we have the public dns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_data = '''#!/bin/bash\n",
    "sudo add-apt-repository ppa:openjdk-r/ppa \n",
    "sudo apt-get update \n",
    "sudo apt-get install -y openjdk-8-jdk\n",
    "wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz -P ~/Downloads\n",
    "sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local\n",
    "sudo mv /usr/local/hadoop-* /usr/local/hadoop\n",
    "\n",
    "sudo cat << EOF >> /home/ubuntu/.profile\n",
    "export JAVA_HOME=/usr\n",
    "export PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNJAVA_HOME/bin\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export PATH=DOLLAR_SIGNPATH:DOLLAR_SIGNHADOOP_HOME/bin\n",
    "export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop\n",
    "EOF\n",
    "\n",
    "sed -i -e 's/DOLLAR_SIGN/$/g' /home/ubuntu/.profile\n",
    "\n",
    "source /home/ubuntu/.profile\n",
    "echo \"export JAVA_HOME=/usr\" >> $HADOOP_CONF_DIR/hadoop-env.sh\n",
    "\n",
    "sudo sed -i -e \"/configuration>/d\" $HADOOP_CONF_DIR/core-site.xml\n",
    "sudo cat << EOF >> $HADOOP_CONF_DIR/core-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://NAMENODE_PUBLIC_DNS:9000</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "EOF\n",
    "\n",
    "sudo sed -i -e \"/configuration>/d\" $HADOOP_CONF_DIR/yarn-site.xml\n",
    "sudo sed -i -e \"/YARN configuration properties/d\" $HADOOP_CONF_DIR/yarn-site.xml\n",
    "sudo cat << EOF >> $HADOOP_CONF_DIR/yarn-site.xml\n",
    "<configuration>\n",
    "<!-- Site specific YARN configuration properties -->\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property> \n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>NAMENODE_PUBLIC_DNS</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "EOF\n",
    "\n",
    "sudo cp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-site.xml\n",
    "sudo sed -i -e \"/configuration>/d\" $HADOOP_CONF_DIR/mapred-site.xml\n",
    "sudo cat << EOF >> $HADOOP_CONF_DIR/mapred-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.jobtracker.address</name>\n",
    "    <value>NAMENODE_PUBLIC_DNS:54311</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "EOF\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Send out request to create instances, finally!!\n",
    "\n",
    "For more details on other options (for example spot instances), pls refer the boto3 documentation linked below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now create instances\n",
    "instances = ec2.create_instances(ImageId = ubuntu14lts,\n",
    "                                 SecurityGroups = [ security_group_name ],\n",
    "                                 InstanceType = instance_type, \n",
    "                                 KeyName = key_pair_name,\n",
    "                                 MinCount = num_nodes,\n",
    "                                 MaxCount = num_nodes,\n",
    "                                 UserData = user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check this a couple of times until all are running/or terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-080f8e2a03ebde2be is running\n",
      "i-0d0ddc30606e9c99c is running\n",
      "i-0ed39d968db0c0e2e is running\n",
      "i-00e466e926beb2587 is running\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BE CAUTIOUS!! This will kill the whole cluster\n",
    "terminate(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-107-20-1-214.compute-1.amazonaws.com\n",
      "ec2-54-210-60-253.compute-1.amazonaws.com\n",
      "ec2-107-20-27-163.compute-1.amazonaws.com\n",
      "ec2-54-145-8-225.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# we need a bunch of parameters that will be needed below\n",
    "# We will set the first node to be the master, and others are slaves\n",
    "pub_dns = []\n",
    "ins_ids = []\n",
    "pri_dns = []\n",
    "for instance in instances:\n",
    "    #print(\"{} is {}\".format(instance.id, ec2.Instance(id=instance.id).state['Name']))\n",
    "    pub_dns.append(ec2.Instance(id=instance.id).public_dns_name)\n",
    "    ins_ids.append(instance.id)\n",
    "    pri_dns.append(ec2.Instance(id=instance.id).private_dns_name.split('.')[0])\n",
    "    print(ec2.Instance(id=instance.id).public_dns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Now, we need to do a bunch of modifications to the hadoop config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'ec2-107-20-1-214.compute-1.amazonaws.com,107.20.1.214' (ECDSA) to the list of known hosts.\n",
      "jupyter.pem                                   100% 1670     1.6KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "# first, upload a copy of pem file to master\n",
    "!scp -i {kp_path} -o 'StrictHostKeyChecking no' {kp_path} ubuntu@{pub_dns[0]}:/home/ubuntu/.ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now config ssh for master node, to gain full ssh access to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ssh_cmd: cmd to update ssh\n",
    "\n",
    "pem_key_file = '/home/ubuntu/.ssh/'+key_pair_name+'.pem'\n",
    "ssh_cmd = '''\n",
    "cat << EOF | tee /home/ubuntu/.ssh/config\n",
    "Host namenode\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "'''.format( pub_dns[0], pem_key_file)\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "Host datanode{}\n",
    "  HostName {}\n",
    "  User ubuntu\n",
    "  IdentityFile {}\n",
    "    '''.format(i+1, dns, pem_key_file)\n",
    "ssh_cmd += '''\n",
    "EOF\n",
    "\n",
    "ssh-keygen -f /home/ubuntu/.ssh/id_rsa -t rsa -P ''\n",
    "cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys\n",
    "'''\n",
    "for i,dns in enumerate(pub_dns[1:]):\n",
    "    ssh_cmd += '''\n",
    "    cat /home/ubuntu/.ssh/id_rsa.pub | ssh -o 'StrictHostKeyChecking no' datanode{} 'cat >> /home/ubuntu/.ssh/authorized_keys'\n",
    "    '''.format(i+1)\n",
    "#ssh_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host namenode\n",
      "  HostName ec2-107-20-1-214.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "\n",
      "Host datanode1\n",
      "  HostName ec2-54-210-60-253.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode2\n",
      "  HostName ec2-107-20-27-163.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Host datanode3\n",
      "  HostName ec2-54-145-8-225.compute-1.amazonaws.com\n",
      "  User ubuntu\n",
      "  IdentityFile /home/ubuntu/.ssh/jupyter.pem\n",
      "    \n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\n",
      "Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\n",
      "The key fingerprint is:\n",
      "b7:0f:97:51:af:a5:64:d5:a3:15:27:fa:0c:a7:a7:14 ubuntu@ip-172-31-8-62\n",
      "The key's randomart image is:\n",
      "+--[ RSA 2048]----+\n",
      "|              ...|\n",
      "|             . .+|\n",
      "|            E ooo|\n",
      "|             Ooo.|\n",
      "|        S . +.* o|\n",
      "|         . o B + |\n",
      "|          o + o  |\n",
      "|           +     |\n",
      "|            .    |\n",
      "+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# update the ssh config at master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{ssh_cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update_cmd: cmd to update everybody's hadoop configuration\n",
    "update_cmd = 'source /home/ubuntu/.profile; '\n",
    "update_cmd += 'sudo sed -i \"s/NAMENODE_PUBLIC_DNS/{}/g\" \\$HADOOP_CONF_DIR/core-site.xml;'.format(pub_dns[0])\n",
    "update_cmd += 'sudo sed -i \"s/NAMENODE_PUBLIC_DNS/{}/g\" \\$HADOOP_CONF_DIR/yarn-site.xml;'.format(pub_dns[0])\n",
    "update_cmd += 'sudo sed -i \"s/NAMENODE_PUBLIC_DNS/{}/g\" \\$HADOOP_CONF_DIR/mapred-site.xml;'.format(pub_dns[0])\n",
    "#run_cmds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-107-20-1-214.compute-1.amazonaws.com\n",
      "ec2-54-210-60-253.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-54-210-60-253.compute-1.amazonaws.com,54.210.60.253' (ECDSA) to the list of known hosts.\n",
      "ec2-107-20-27-163.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-107-20-27-163.compute-1.amazonaws.com,107.20.27.163' (ECDSA) to the list of known hosts.\n",
      "ec2-54-145-8-225.compute-1.amazonaws.com\n",
      "Warning: Permanently added 'ec2-54-145-8-225.compute-1.amazonaws.com,54.145.8.225' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "# update each node (include master)\n",
    "for everybody in pub_dns: # slave public dns\n",
    "    print(everybody)\n",
    "    !ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{everybody} \"{update_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now update the master hadoop config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# master_cmd: modify master \n",
    "\n",
    "master_cmd = '''cat << EOF | sudo tee /etc/hosts\n",
    "127.0.0.1 localhost\n",
    "'''\n",
    "for instance in instances:\n",
    "    master_cmd += '{} {}\\n'.format(ec2.Instance(id=instance.id).public_dns_name, \n",
    "                            ec2.Instance(id=instance.id).private_dns_name.split('.')[0])\n",
    "master_cmd += '''\n",
    "# The following lines are desirable for IPv6 capable hosts\n",
    "::1 ip6-localhost ip6-loopback\n",
    "fe00::0 ip6-localnet\n",
    "ff00::0 ip6-mcastprefix\n",
    "ff02::1 ip6-allnodes\n",
    "ff02::2 ip6-allrouters\n",
    "ff02::3 ip6-allhosts\n",
    "EOF\n",
    "\n",
    "source /home/ubuntu/.profile\n",
    "\n",
    "sudo sed -i -e '/configuration>/d' \\$HADOOP_CONF_DIR/hdfs-site.xml\n",
    "sudo cat << EOF | sudo tee --append \\$HADOOP_CONF_DIR/hdfs-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value> {} </value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/namenode</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "EOF\n",
    "\n",
    "sudo mkdir -p \\$HADOOP_HOME/hadoop_data/hdfs/namenode\n",
    "\n",
    "'''.format(num_nodes-1)\n",
    "\n",
    "master_cmd += ' echo \"{}\" | sudo tee \\$HADOOP_CONF_DIR/masters; '.format(pri_dns[0])\n",
    "master_cmd += ' sudo rm -rf \\$HADOOP_CONF_DIR/slaves;'\n",
    "for slave in pri_dns[1:]:\n",
    "    master_cmd += 'sudo echo \"{}\" | sudo tee --append \\$HADOOP_CONF_DIR/slaves; '.format(slave)\n",
    "\n",
    "master_cmd += 'sudo chown -R ubuntu \\$HADOOP_HOME'\n",
    "#master_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 localhost\n",
      "ec2-107-20-1-214.compute-1.amazonaws.com ip-172-31-8-62\n",
      "ec2-54-210-60-253.compute-1.amazonaws.com ip-172-31-12-203\n",
      "ec2-107-20-27-163.compute-1.amazonaws.com ip-172-31-7-232\n",
      "ec2-54-145-8-225.compute-1.amazonaws.com ip-172-31-15-211\n",
      "\n",
      "# The following lines are desirable for IPv6 capable hosts\n",
      "::1 ip6-localhost ip6-loopback\n",
      "fe00::0 ip6-localnet\n",
      "ff00::0 ip6-mcastprefix\n",
      "ff02::1 ip6-allnodes\n",
      "ff02::2 ip6-allrouters\n",
      "ff02::3 ip6-allhosts\n",
      "<configuration>\n",
      "  <property>\n",
      "    <name>dfs.replication</name>\n",
      "    <value> 3 </value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>dfs.namenode.name.dir</name>\n",
      "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/namenode</value>\n",
      "  </property>\n",
      "</configuration>\n",
      "ip-172-31-8-62\n",
      "ip-172-31-12-203\n",
      "ip-172-31-7-232\n",
      "ip-172-31-15-211\n"
     ]
    }
   ],
   "source": [
    "# modify master\n",
    "!ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{pub_dns[0]} \"{master_cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slave_cmd: modify slave config\n",
    "\n",
    "slave_cmd = '''\n",
    "source /home/ubuntu/.profile\n",
    "\n",
    "sudo sed -i -e '/configuration>/d' \\$HADOOP_CONF_DIR/hdfs-site.xml\n",
    "sudo cat << EOF | sudo tee --append \\$HADOOP_CONF_DIR/hdfs-site.xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value> {} </value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "EOF\n",
    "\n",
    "sudo mkdir -p \\$HADOOP_HOME/hadoop_data/hdfs/datanode\n",
    "\n",
    "sudo chown -R ubuntu \\$HADOOP_HOME\n",
    "\n",
    "'''.format(num_nodes-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-54-210-60-253.compute-1.amazonaws.com\n",
      "<configuration>\n",
      "  <property>\n",
      "    <name>dfs.replication</name>\n",
      "    <value> 3 </value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>dfs.datanode.data.dir</name>\n",
      "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>\n",
      "  </property>\n",
      "</configuration>\n",
      "ec2-107-20-27-163.compute-1.amazonaws.com\n",
      "<configuration>\n",
      "  <property>\n",
      "    <name>dfs.replication</name>\n",
      "    <value> 3 </value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>dfs.datanode.data.dir</name>\n",
      "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>\n",
      "  </property>\n",
      "</configuration>\n",
      "ec2-54-145-8-225.compute-1.amazonaws.com\n",
      "<configuration>\n",
      "  <property>\n",
      "    <name>dfs.replication</name>\n",
      "    <value> 3 </value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>dfs.datanode.data.dir</name>\n",
      "    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>\n",
      "  </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "# modify each slave\n",
    "for slave in pub_dns[1:]: # slave public dns\n",
    "    print(slave)\n",
    "    !ssh -i {kp_path} -o \"StrictHostKeyChecking no\" \"ubuntu@\"{slave} \"{slave_cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurrah! You are ready to run your hadoop cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use your termininal to start the cluster <br>\n",
    "Open a terminal, ssh to your master node <br>\n",
    "And run the commands:\n",
    "\n",
    "```bash\n",
    "master$ hdfs namenode -format\n",
    "master$ $HADOOP_HOME/sbin/start-dfs.sh\n",
    "```\n",
    "\n",
    "For more details, check out Ouyang's post: <br>\n",
    "https://blog.insightdatascience.com/spinning-up-a-free-hadoop-cluster-step-by-step-c406d56bae42#.j3zrw1yua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to this place to find the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ec2-107-20-1-214.compute-1.amazonaws.com:50070'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://{}:50070'.format(pub_dns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terminate(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-080f8e2a03ebde2be is terminated\n",
      "i-0d0ddc30606e9c99c is terminated\n",
      "i-0ed39d968db0c0e2e is terminated\n",
      "i-00e466e926beb2587 is terminated\n"
     ]
    }
   ],
   "source": [
    "check_status(ec2, instances)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsny]",
   "language": "python",
   "name": "conda-env-dsny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
